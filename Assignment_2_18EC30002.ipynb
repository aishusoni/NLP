{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "# Assignment 2 on Natural Language Processing\n",
    "\n",
    "### Date : 15th Sept, 2020\n",
    "\n",
    "### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "The central idea of this assignment is to make you familiar with programming in python and also the language modelling task of natural language processing using the python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stk58juYkzEr"
   },
   "source": [
    "**Dataset:** \n",
    "\n",
    " Use the text file provided along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "tC9c4650poRo",
    "outputId": "c18708d1-88f7-46d3-d331-2960473ed981"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "I:\\Anaconda\\python.exe -m pip install --upgrade pip\n",
      "You are using pip version 10.0.1, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "#!pip install --upgrade pip\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "Hdbm9Usopwk0",
    "outputId": "24928a26-0b0b-435f-ec38-9b9c177bc5ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\aishwarya\n",
      "[nltk_data]     soni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\aishwarya\n",
      "[nltk_data]     soni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read dataset\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUj8j_gJAxv5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_corp = open(\"corpus1.txt\",\"r+\",encoding=\"utf8\")\n",
    "corpus1 = []\n",
    "for li in text_corp:\n",
    "  corpus1.append(li)\n",
    "text = \"\".join(corpus1)\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "rT6byv49kdmo",
    "outputId": "ed41192f-91f9-4e43-ba55-f0fcf409eb77",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRGqKaDn1pJy"
   },
   "source": [
    "\n",
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "C1OtHn6B1oc2",
    "outputId": "28c4c1f3-f8b9-474c-af72-0d50bb4b447a"
   },
   "outputs": [],
   "source": [
    "tokens = sent_tokenize(text)\n",
    "for e in range(len(tokens)):\n",
    "  tokens[e] = tokens[e].lower() \n",
    "  tokens[e] = re.sub(r\"[^a-zA-Z0-9 ]+\", \"\", tokens[e])\n",
    "  tokens[e] = word_tokenize(tokens[e])\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDL7yfpXkMRS"
   },
   "source": [
    "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
    "\n",
    "1. **Create the following language models** on the training corpus: <br>\n",
    "    i.   Unigram <br>\n",
    "    ii.  Bigram <br>\n",
    "    iii. Trigram <br>\n",
    "    iv.  Fourgram <br>\n",
    "\n",
    "2. **List the top 5 bigrams, trigrams, four-grams (with and without Add-1 smoothing).**\n",
    "(Note: Please remove those which contain only articles, prepositions, determiners. For Example: “of the”, “in a”, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "u3oIulBikPua",
    "outputId": "59d0c63d-be02-406f-bb50-37e874215513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 unigrams: \n",
      "-the\n",
      "-and\n",
      "-to\n",
      "-a\n",
      "-she\n",
      "Top 5 bigrams: \n",
      "('said', 'the')\n",
      "('of', 'the')\n",
      "('said', 'alice')\n",
      "('in', 'a')\n",
      "('and', 'the')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mI:\\Anaconda\\lib\\site-packages\\nltk\\util.py\u001b[0m in \u001b[0;36mngrams\u001b[1;34m(sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-02cb516c03c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrigrams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mfdist03\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrigrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Top 5 trigrams: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "#Write code\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "unigrams=[]\n",
    "bigrams=[]\n",
    "trigrams=[]\n",
    "fourgrams=[]\n",
    "\n",
    "for content in tokens:\n",
    "    unigrams.extend(content)\n",
    "for content in tokens:\n",
    "    bigrams.extend(ngrams(content,2))\n",
    "    \n",
    "fdist01 = nltk.FreqDist(unigrams)\n",
    "print(\"Top 5 unigrams: \")\n",
    "for word, frequency in fdist01.most_common(5):\n",
    "    print('-'+ word)\n",
    "\n",
    "fdist02 = nltk.FreqDist(bigrams)\n",
    "print(\"Top 5 bigrams: \")\n",
    "for word, frequency in fdist02.most_common(5):\n",
    "    print( word)\n",
    "\n",
    "for content in tokens:\n",
    "    trigrams.extend(ngrams(content,3))\n",
    "fdist03 = nltk.FreqDist(trigrams)\n",
    "print(\"Top 5 trigrams: \")\n",
    "for word, frequency in fdist03.most_common(5):\n",
    "    print( word)\n",
    "\n",
    "for content in tokens:\n",
    "    fourgrams.extend(ngrams(content,4))\n",
    "fdist04 = nltk.FreqDist(fourgrams)\n",
    "print(\"Top 5 fourgrams: \")\n",
    "for word, frequency in fdist04.most_common(5):\n",
    "    print( word)\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "#i was not able to update/modify the pips beacause of which trrigrams and fourgrams are showing the following error\n",
    "#..maybe in ur device it might work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "colab_type": "code",
    "id": "vARsvSfynePr",
    "outputId": "8757fe52-68dc-4990-888a-76f7856cf65c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing all the stopwords\n",
      "\n",
      "word sequence:  said  Count:  462\n",
      "word sequence:  alice  Count:  385\n",
      "word sequence:  little  Count:  128\n",
      "word sequence:  one  Count:  101\n",
      "word sequence:  like  Count:  85\n",
      "word sequence:  know  Count:  85\n",
      "word sequence:  would  Count:  83\n",
      "word sequence:  went  Count:  83\n",
      "word sequence:  could  Count:  77\n",
      "word sequence:  thought  Count:  74\n",
      "\n",
      "word sequence:  ('said', 'alice')  Count:  230\n",
      "word sequence:  ('said', 'the')  Count:  209\n",
      "word sequence:  ('mock', 'turtle')  Count:  108\n",
      "word sequence:  ('the', 'queen')  Count:  65\n",
      "word sequence:  ('march', 'hare')  Count:  62\n",
      "word sequence:  ('the', 'king')  Count:  60\n",
      "word sequence:  ('a', 'little')  Count:  59\n",
      "word sequence:  ('the', 'mock')  Count:  53\n",
      "word sequence:  ('the', 'gryphon')  Count:  53\n",
      "word sequence:  ('thought', 'alice')  Count:  52\n",
      "\n",
      "word sequence:  ('cats', 'eat', 'bats')  Count:  9\n",
      "word sequence:  ('do', 'cats', 'eat')  Count:  6\n",
      "word sequence:  ('pictures', 'or', 'conversations')  Count:  4\n",
      "word sequence:  ('book', 'thought', 'alice')  Count:  3\n",
      "word sequence:  ('thought', 'alice', 'without')  Count:  3\n",
      "word sequence:  ('alice', 'without', 'pictures')  Count:  3\n",
      "word sequence:  ('hot', 'day', 'made')  Count:  3\n",
      "word sequence:  ('pink', 'eyes', 'ran')  Count:  3\n",
      "word sequence:  ('eyes', 'ran', 'close')  Count:  3\n",
      "word sequence:  ('seemed', 'quite', 'natural')  Count:  3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#stopwords = code for downloading stop words through nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"After removing all the stopwords\")\n",
    "print()\n",
    "#print top 10 unigrams, bigrams after removing stopwords\n",
    "\n",
    "uni_processed = [gram for gram in unigrams if gram not in stop_words]\n",
    "fdist1 = nltk.FreqDist(uni_processed)\n",
    "k = Counter(fdist1)\n",
    "ten = k.most_common(10)\n",
    "for elem in ten:\n",
    "  print(\"word sequence: \",elem[0],\" Count: \",elem[1])\n",
    "print()\n",
    "\n",
    "bi_processed = []\n",
    "tri_processed = []\n",
    "four_processed = []\n",
    "for gram in bigrams:\n",
    "  for t in gram:\n",
    "    if t not in stop_words:\n",
    "      bi_processed.append(gram)\n",
    "\n",
    "fdist2 = nltk.FreqDist(bi_processed)\n",
    "l= Counter(fdist2)\n",
    "bi_ten = l.most_common(10)\n",
    "for elem in bi_ten:\n",
    "  print(\"word sequence: \",elem[0],\" Count: \",elem[1])\n",
    "print()\n",
    "\n",
    "for gram in trigrams:\n",
    "  for t in gram:\n",
    "    if t not in stop_words:\n",
    "      tri_processed.append(gram)\n",
    "# tri_processed = [gram for gram in trigrams if gram not in stop_words]\n",
    "fdist3 = nltk.FreqDist(tri_processed)\n",
    "k = Counter(fdist3)\n",
    "tri_ten = k.most_common(10)\n",
    "for elem in tri_ten:\n",
    "  print(\"word sequence: \",elem[0],\" Count: \",elem[1])\n",
    "print()\n",
    "\n",
    "for gram in fourgrams:\n",
    "  for t in gram:\n",
    "    if t not in stop_words:\n",
    "      four_processed.append(gram)\n",
    "# uni_processed = [gram for gram in unigrams if gram not in stop_words]\n",
    "fdist4 = nltk.FreqDist(four_processed)\n",
    "k = Counter(fdist4)\n",
    "four_ten = k.most_common(10)\n",
    "for elem in four_ten:\n",
    "  print(\"word sequence: \",elem[0],\" Count: \",elem[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ioc1xNjmnim-"
   },
   "source": [
    "# Applying Smoothing\n",
    "\n",
    "\n",
    "Assume additional training data in which each possible N-gram occurs exactly once and adjust estimates.\n",
    "\n",
    "> ### $ Probability(ngram) = \\frac{Count(ngram)+1}{ N\\, +\\, V} $\n",
    "\n",
    "N: Total number of N-grams <br>\n",
    "V: Number of unique N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "colab_type": "code",
    "id": "grh4sO0Yns4V",
    "outputId": "cd9e5772-3ce2-4aea-8360-25894df0364d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdist03' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fe977b6c42c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mtrigram_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mV3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdist03\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mN3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrigrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfdist03\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fdist03' is not defined"
     ]
    }
   ],
   "source": [
    "#You are to perform Add-1 smoothing here:\n",
    "\n",
    "V1 = len(fdist01.keys())\n",
    "N1 = len(unigrams)\n",
    "unigram_1 = {}\n",
    "for key in fdist01.keys():\n",
    "  unigram_1[key] = (fdist01[key]+1)/(N1+V1)\n",
    "  \n",
    "#write similar code for bigram, trigram and fourgrams\n",
    "bigram_1 = {}\n",
    "V2 = len(fdist02.keys())\n",
    "N2 = len(bigrams)\n",
    "for key in fdist02.keys():\n",
    "  bigram_1[key] = (fdist02[key]+1)/(N2+V2)\n",
    "\n",
    "trigram_1={}\n",
    "V3 = len(fdist03.keys())\n",
    "N3 = len(trigrams)\n",
    "for key in fdist03.keys():\n",
    "  trigram_1[key] = (fdist03[key]+1)/(N3+V3)\n",
    "\n",
    "fourgram_1 = {}\n",
    "V4 = len(fdist04.keys())\n",
    "N4 = len(fourgrams)\n",
    "for key in fdist04.keys():\n",
    "  fourgram_1[key] = (fdist04[key]+1)/(N4+V4)\n",
    "\n",
    "#Print top 10 unigram, bigram, trigram, fourgram after smoothing\n",
    "k = Counter(unigram_1)\n",
    "ten1 = k.most_common(10)\n",
    "for el in ten1:\n",
    "  print(\"word sequence: \",el[0],\" and Probability: \",el[1])\n",
    "print()\n",
    "l = Counter(bigram_1)\n",
    "ten2 = l.most_common(10)\n",
    "for el in ten2:\n",
    "  print(\"word sequence: \",el[0],\" and Probability: \",el[1])\n",
    "print()\n",
    "m = Counter(trigram_1)\n",
    "ten3 = m.most_common(10)\n",
    "for el in ten3:\n",
    "  print(\"word sequence: \",el[0],\" and Probability: \",el[1])\n",
    "print()\n",
    "n = Counter(fourgram_1)\n",
    "ten4 = n.most_common(10)\n",
    "for el in ten4:\n",
    "  print(\"word sequence: \",el[0],\" and Probability: \",el[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0GL40mQmmt4"
   },
   "source": [
    "### Predict the next word using statistical language modelling\n",
    "\n",
    "Using the above bigram, trigram, and fourgram models that you just experimented with, **predict the next word(top 5 probable) given the previous n(=2, 3, 4)-grams** for the sentences below.\n",
    "\n",
    "For str1, str2, you are to predict the next  2 possible word sequences using your trained smoothed models. <br> \n",
    "For example, for the string 'He looked very' the answers can be as below: \n",
    ">     (1) 'He looked very' *anxiouxly* \n",
    ">     (2) 'He looked very' *uncomfortable* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBWKo5_Fmnbg"
   },
   "outputs": [],
   "source": [
    "str1 = 'after that alice said the'\n",
    "str2 = 'alice felt so desperate that she was'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "colab_type": "code",
    "id": "ext_nVn2mvZt",
    "outputId": "71c54826-2c2c-4c42-f192-c1b696e17995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Str1 predictions:\n",
      "\n",
      "Top-5 Bigram :\n",
      "queen   0.02996934520776446\n",
      "king   0.0276989402677823\n",
      "mock   0.024520373351807286\n",
      "gryphon   0.024520373351807286\n",
      "hatter   0.023612211375814426\n",
      "\n",
      "\n",
      "Str2 predictions:\n",
      "\n",
      "Top-5 Bigram :\n",
      "a   0.06619942716622831\n",
      "the   0.04344337407783733\n",
      "not   0.026893517286280255\n",
      "that   0.02482478518733562\n",
      "going   0.02482478518733562\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('she', 'was', 'to')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e4b82fa7b033>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword32\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrigrams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mpr32\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrigram_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbigram_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword42\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfourgrams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ('she', 'was', 'to')"
     ]
    }
   ],
   "source": [
    "#write code\n",
    "st1 = word_tokenize(str1)\n",
    "st2 = word_tokenize(str2)\n",
    "\n",
    "word21 = st1[-1:]\n",
    "word31 =tuple(st1[-2:])\n",
    "word41 = tuple(st1[-3:])\n",
    "\n",
    "word22 = st2[-1:]\n",
    "word32 =tuple(st2[-2:])\n",
    "word42 = tuple(st2[-3:])\n",
    "\n",
    "pr21={}\n",
    "pr31={}\n",
    "pr41={}\n",
    "pr22={}\n",
    "pr32={}\n",
    "pr42={}\n",
    "\n",
    "for key in fdist01.keys():\n",
    "  word = (word21[0],)+(key,)\n",
    "  if word in bigrams:\n",
    "    pr21[key] = bigram_1[word]/unigram_1[word21[0]]\n",
    "  word = (word22[0],)+ (key,)\n",
    "  if word in bigrams:\n",
    "    pr22[key] = bigram_1[word]/unigram_1[word22[0]]\n",
    "  \n",
    "\n",
    "print(\"Str1 predictions:\" )\n",
    "print()\n",
    "print(\"Top-5 Bigram :\")\n",
    "ans = Counter(pr21)\n",
    "five21 = ans.most_common(5)\n",
    "for elem in five21:\n",
    "  print(elem[0],\" \",elem[1])\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Str2 predictions:\" )\n",
    "print()\n",
    "print(\"Top-5 Bigram :\")\n",
    "ans = Counter(pr22)\n",
    "five22 = ans.most_common(5)\n",
    "for elem in five22:\n",
    "  print(elem[0],\" \",elem[1])\n",
    "print()\n",
    "\n",
    "\n",
    "for key in fdist01.keys():\n",
    "  word = word31+(key,)\n",
    "  if word in trigrams:\n",
    "    pr31[key] = trigram_1[word]/bigram_1[word31]\n",
    "  word = word41+(key,)\n",
    "  if word in fourgrams:\n",
    "    pr41[key] = fourgram_1[word]/trigram_1[word41]\n",
    "  word = word32+(key,)\n",
    "  if word in trigrams:\n",
    "    pr32[key] = trigram_1[word]/bigram_1[word32]\n",
    "  word = word42+(key,)\n",
    "  if word in fourgrams:\n",
    "    pr42[key] = fourgram_1[word]/trigram_1[word42]\n",
    "\n",
    "\n",
    "print(\"Top-5 Trigram :\")\n",
    "ans = Counter(pr31)\n",
    "five31 = ans.most_common(5)\n",
    "for elem in five31:\n",
    "  print(elem[0],\" \",elem[1])\n",
    "print()\n",
    "\n",
    "print(\"Top-5 Fourgram :(this case is null)\")\n",
    "ans = Counter(pr41)\n",
    "five41 = ans.most_common(5)\n",
    "for elem in five41:\n",
    "  print(elem[0],\" \",elem[1])\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Top-5 Trigram :\")\n",
    "ans = Counter(pr32)\n",
    "five32 = ans.most_common(5)\n",
    "for elem in five32:\n",
    "  print(elem[0],\" \",elem[1])\n",
    "print()\n",
    "\n",
    "print(\"Top-5 Fourgram :\")\n",
    "ans = Counter(pr42)\n",
    "five42 = ans.most_common(5)\n",
    "for elem in five42:\n",
    "  print(elem[0],\" \",elem[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWYPM49lqYaI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Assgnmt_2_18EC30002.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
